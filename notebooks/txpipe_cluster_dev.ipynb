{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35818ae3-26f3-48cf-b910-a22cae198f23",
   "metadata": {},
   "source": [
    "# TXPipe - CLMM Data Preparation\n",
    "\n",
    "This notebook runs and explores two pipelines that generate the weighted, calibrated, per-cluster background shear catalogs as inputs to CLMM.  The instructions for running this on IN2P3 and on NERSC will differ.\n",
    "\n",
    "On IN2P3 -- before starting you will need to:\n",
    "- set up the TXPipe environment at CC-IN2p3 using the command \n",
    "`source /pbs/throng/lsst/users/jzuntz/txpipe-environments/setup-txpipe`\n",
    "- clone the TXPipe repository somewhere. Note, the recommendation is for big data files to live in the sps folder, but not code.  You will want additional data (not tracked by git) to live in sps, and to softlink to the data directory in TXPipe. There are potentially big data files in the the data folder inside TXPipe, but these should ideally be put  in the SPS space since you may generate large files.\n",
    "- downloaded the two input catalogs: [1 square degree](https://portal.nersc.gov/cfs/lsst/txpipe/data/example.tar.gz) and [20 square degrees](https://portal.nersc.gov/cfs/lsst/txpipe/data/cosmodc2-20deg2.tar.gz) and unzipped them in your TXPipe clone directory.\n",
    "\n",
    "On NERSC -- make sure you work through the installation and data downloading instructions on the README:\n",
    "- [Install TXPipe](https://github.com/LSSTDESC/TXPipe#installing)\n",
    "- [Download](https://github.com/LSSTDESC/TXPipe#running) example data\n",
    "\n",
    "You should then be able to execute the cells below in the **1 deg$^2$ Sample** section with the TXPipe kernel in NERSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c35f16-03a7-4042-bc68-8858711d8974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import ceci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d52caf-ba4f-4d87-98fc-36bbd275135e",
   "metadata": {},
   "source": [
    "# 1 deg$^2$ Sample\n",
    "\n",
    "First we will do some runs on the 1 deg^2 example data set with around 80k galaxies. This is small enough that we can do it all in jupyter.\n",
    "\n",
    "The data set, which is based on CosmoDC2, contains pre-computed photo-z and and contains a RedMapper cluster catalog for the field.\n",
    "\n",
    "We will clone our own copy of the TXPipe directory, and run this notebook from there.  **Please change `my_txpipe_dir`** to your own version of the path when running this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32961686-f5c2-4873-b529-9fcaf22c54e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# my_txpipe_dir = \"/pscratch/sd/a/avestruz/TXPipe\"\n",
    "my_txpipe_dir = \"/pbs/throng/lsst/users/ccombet/TXPipe\"\n",
    "os.chdir(my_txpipe_dir)\n",
    "\n",
    "import txpipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fd3ce-e568-4228-9788-436c36916d36",
   "metadata": {},
   "source": [
    "Now we make an output directory for everything, if it doesn't exist already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17705a-f043-41f6-97a7-6bae04237781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"data/example/outputs_metadetect\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2123975-cf19-4f1c-bae4-6894ecd42d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/example/inputs/metadetect_shear_catalog.hdf5\"):\n",
    "    raise RuntimeError(\"Download and extract the sample data file to continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5c54a-c9f2-4301-ba88-29dff76f93a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## WL sample selection\n",
    "\n",
    "Our first step is the WL sample selection. This does both selection and tomography. The latter is not used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed600f7-4cf7-4eff-9ba6-2edb1ead85c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step1 = txpipe.TXSourceSelectorMetadetect.make_stage(\n",
    "    # This file is the input metadetect shear catalog\n",
    "    shear_catalog=\"data/example/inputs/metadetect_shear_catalog.hdf5\",\n",
    "    # This is an input training set for the tomographic selection\n",
    "    calibration_table=\"data/example/inputs/sample_cosmodc2_w10year_errors.dat\",\n",
    "\n",
    "    # This contains all the options for this stage. You can override them here\n",
    "    # manually too.\n",
    "    config=\"examples/metadetect/config.yml\",\n",
    "\n",
    "    # This is the output file for this stage\n",
    "    shear_tomography_catalog=\"data/example/outputs_metadetect/shear_tomography_catalog.hdf5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b30b542-fe35-478d-9561-f4df45d83850",
   "metadata": {},
   "source": [
    "This step will first train a classifier to select objects into tomographic bins, and then run it on the input data files\n",
    "to produce the output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9def04-7ae8-4f39-80b6-8f18d4faaf11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step1.run()\n",
    "step1.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33844c37-b2c7-4400-9f6c-f0c5bb0c3ce4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cluster shear catalog indexing and weights\n",
    "\n",
    "Our second step runs the matching to find the shear catalog behind every cluster.\n",
    "\n",
    "This step saves a cluster shear catalog, which is actually just an index into the shear and cluster catalogs (to avoid making many copies of the data), with added weights from CLMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd1f06-b0ea-4afb-b398-0ef5404d8ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Options for this pipeline and their defaults:\")\n",
    "print(txpipe.extensions.CLClusterShearCatalogs.config_options)\n",
    "\n",
    "step2 = txpipe.extensions.CLClusterShearCatalogs.make_stage(\n",
    "    # Shear catalog, as before\n",
    "    shear_catalog=\"data/example/inputs/metadetect_shear_catalog.hdf5\",\n",
    "    # This is the initial cluster catalog - RAs, Decs, richess, redshift, etc.\n",
    "    cluster_catalog=\"./data/example/inputs/cluster_catalog.hdf5\",\n",
    "    # This fiducial cosmology is used to convert distance separations to redshifts\n",
    "    fiducial_cosmology=\"./data/fiducial_cosmology.yml\",\n",
    "    # The tomography catalog created in step 1 selects objects for the WL sample\n",
    "    # and assigns them to tomographic bins. We don't need the tomography here, just the basic selection\n",
    "    shear_tomography_catalog=\"data/example/outputs_metadetect/shear_tomography_catalog.hdf5\",\n",
    "    # This is a QP file created by RAIL to generate the photo-zs for this sample\n",
    "    source_photoz_pdfs=\"data/example/inputs/photoz_pdfs.hdf5\",\n",
    "\n",
    "    # This is the output for this stage\n",
    "    cluster_shear_catalogs=\"my_cluster_shear_catalog.hdf5\",\n",
    "\n",
    "    # This contains all the options for this stage. You can override them here, as we do with the max_radius below.\n",
    "    config=\"examples/metadetect/config.yml\",    \n",
    "    # Let's override one of the configuration parameters for this stage:\n",
    "    max_radius=5.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdc1eb8-4617-4848-b40d-85a3fdd2d6be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step2.run()\n",
    "step2.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a88793-56c1-4988-a52c-17496fd04546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step3 = txpipe.extensions.CLClusterEnsembleProfiles.make_stage(\n",
    "\n",
    "        # Shear catalog, as before\n",
    "    shear_catalog=\"data/example/inputs/metadetect_shear_catalog.hdf5\",\n",
    "    # This is the initial cluster catalog - RAs, Decs, richess, redshift, etc.\n",
    "    cluster_catalog=\"./data/example/inputs/cluster_catalog.hdf5\",\n",
    "    # This fiducial cosmology is used to convert distance separations to redshifts\n",
    "    fiducial_cosmology=\"./data/fiducial_cosmology.yml\",\n",
    "    # The tomography catalog created in step 1 selects objects for the WL sample\n",
    "    # and assigns them to tomographic bins. We don't need the tomography here, just the basic selection\n",
    "    shear_tomography_catalog=\"data/example/outputs_metadetect/shear_tomography_catalog.hdf5\",\n",
    "    # This is a QP file created by RAIL to generate the photo-zs for this sample\n",
    "    source_photoz_pdfs=\"data/example/inputs/photoz_pdfs.hdf5\",\n",
    "    cluster_shear_catalogs=\"my_cluster_shear_catalog.hdf5\",    \n",
    "    # This is the output for this stage\n",
    "    cluster_profiles=\"./my_cluster_ensemble_object.hdf5\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda9417-b3b1-4e70-acd8-7b34adad1d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "step3.run()\n",
    "step3.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7d1e3-186b-42b8-ad4f-d96b59781a36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring the index\n",
    "\n",
    "To avoid making lots and lots of copies of the data, this stage has not made a catalog, but instead made an index into the other catalogs, and stored only the relevant weight.\n",
    "\n",
    "We have a helper class which is designed to match up all the different catalogs that go into this and collect the results for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14189ad-0523-4603-8ae2-e5cf44f2da34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ccc = txpipe.extensions.CombinedClusterCatalog(\n",
    "    shear_catalog=\"data/example/inputs/metadetect_shear_catalog.hdf5\",\n",
    "    shear_tomography_catalog=\"data/example/outputs_metadetect/shear_tomography_catalog.hdf5\",\n",
    "    cluster_catalog=\"./data/example/inputs/cluster_catalog.hdf5\",\n",
    "    cluster_shear_catalogs=\"my_cluster_shear_catalog.hdf5\",\n",
    "    photoz_pdfs=\"data/example/inputs/photoz_pdfs.hdf5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23625e43-eab2-4ae5-ad1b-1122ec00608c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Have {ccc.ncluster} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40271313-fc70-46e1-b502-45af5518a1a6",
   "metadata": {},
   "source": [
    "We can extract the cluster catalog info by index (0 -- 74):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec84c1-20b7-4478-be3d-b214632a3680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_info = ccc.get_cluster_info(0)\n",
    "cluster_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2616e035-a282-4f5e-8cda-85fa426bddd4",
   "metadata": {},
   "source": [
    "A also the shear catalog associated with that cluster, again by index, in the CLMM data format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa1f65-eece-47c6-a50d-89c24ca59a2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bg_cat = ccc.get_background_shear_catalog(0)\n",
    "bg_cat[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97925489-6fec-4257-86c8-e29ba5f6d9e8",
   "metadata": {},
   "source": [
    "Since our field is so small here (1 deg^2) the background catalog may be cut off at the edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a027ef4-f435-4a02-8c7f-56a931016bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(bg_cat['ra'], bg_cat['dec'], c=bg_cat['distance_arcmin'], s=1)\n",
    "plt.plot(cluster_info['ra'], cluster_info['dec'], 'r*', markersize=10)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7cc171-ab5a-41cf-8d5d-5d3b0e363554",
   "metadata": {},
   "source": [
    "We can also look at the redshift-weight calculation result for this b/g sample to check it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ebb3d-58de-4524-b2ec-7f2a592caa31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(bg_cat['zmean'], bg_cat['weight_clmm'], ',')\n",
    "plt.xlabel(\"Redshift of b/g galaxy\")\n",
    "plt.ylabel(\"CLMM weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc8b58-0241-438e-a83e-f87e2507ac71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#radii2 = (bg_cat['ra'] - cl_cat['ra'])**2 + ((bg_cat['dec'] - cl_cat['dec'])**2)\n",
    "\n",
    "plt.scatter(bg_cat[\"distance_arcmin\"], bg_cat[\"tangential_comp_clmm\"], marker='.')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Radius [arcmin]\")\n",
    "plt.ylabel(\"Tangential Delta Sigma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34bbe6-e101-4afa-8bda-cc53c08a41bf",
   "metadata": {},
   "source": [
    "# 20 deg$^2$ Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733d99a-b0e0-46dd-ab86-c46bcb1e692b",
   "metadata": {},
   "source": [
    "Our second input catalog contains a larger data set - 20 square degrees of CosmoDC2 data + mock noise, and an accompanying redmapper cluster catalog and mock spectroscopic sample.\n",
    "\n",
    "It contains about 25 million galaxies and 1900 clusters.\n",
    "\n",
    "This is large enough that it's worth running in parallel, instead of in Jupyter, especially because we have to calculate the photo-z for this sample, which is pretty slow.\n",
    "\n",
    "To download the 20 deg$^2$ catalog (13 GB), \n",
    "```\n",
    "curl -O https://portal.nersc.gov/cfs/lsst/txpipe/data/cosmodc2-20deg2.tar.gz\n",
    "tar -zxvf cosmodc2-20deg2.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8bbc69-0c16-4158-905b-34c4c28db967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/cosmodc2/20deg2/cluster_catalog.hdf5\"):\n",
    "    raise RuntimeError(\"Download and extract the 20 deg^2 data file to continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be3193-7583-4630-a268-b875bdd6a3cf",
   "metadata": {},
   "source": [
    "## Launching a pipeline\n",
    "\n",
    "At CC-IN2P3: Let's have a look at the submission script for this pipeline: `examples/cosmodc2/20deg2-in2p3.sub`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708a796-f6b6-4fbc-b7ac-b852b8468a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat examples/cosmodc2/20deg2-in2p3.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830a67c-a393-4061-b08b-697d7ad711de",
   "metadata": {},
   "source": [
    "At NERSC: Let's have a look at the submission script for this pipeline: `examples/cosmodc2/20deg2-nersc.sub`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011471cc-c43a-4bac-9395-7bfe060883ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat examples/cosmodc2/20deg2-nersc.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca15a17-07ed-4f08-bac3-7440e3d91442",
   "metadata": {},
   "source": [
    "These submission scripts will launch a job of up to one hour (it should finish in 30 min) on a single node in CC-IN2P3 (for the former) or NERSC perlmutter (for the latter) to run a pipeline.\n",
    "\n",
    "In a terminal, **navigate to your TXPipe directory and run the following to launch the job**\n",
    "\n",
    "In IN2P3:\n",
    "```\n",
    "sbatch examples/cosmodc2/20deg2-in2p3.sub\n",
    "```\n",
    "\n",
    "In NERSC:\n",
    "```\n",
    "sbatch examples/cosmodc2/20deg2-nersc.sub\n",
    "```\n",
    "\n",
    "## Investigating our pipeline\n",
    "\n",
    "While that's running, let's have a look at what's in the pipeline.\n",
    "\n",
    "- The pipeline file on IN2P3 is `examples/cosmodc2/pipeline-20deg2-clmm.yml`\n",
    "- The pipeline file on NERSC is `examples/cosmodc2/pipeline-20deg2-clmm-nersc.yml`\n",
    "\n",
    "Comment/uncomment out the appropriate `pipeline_file` definition below, depending on whether you are running on IN2P3 or on NERSC\n",
    "\n",
    "First, we can use ceci to build a flow-chart showing the pipeline stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a6376a-50aa-4cfc-ad2b-c2d6291d292b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the appropriate pipeline configuration, and ask for a flow-chart.\n",
    "# pipeline_file = \"examples/cosmodc2/pipeline-20deg2-clmm.yml\"\n",
    "# pipeline_file = \"examples/cosmodc2/pipeline-20deg2-clmm-nersc.yml\"\n",
    "pipeline_file = \"examples/cosmodc2/pipeline-20deg2-CLdev.yml\"\n",
    "\n",
    "flowchart_file = \"20deg2.png\"\n",
    "\n",
    "\n",
    "pipeline_config = ceci.Pipeline.build_config(\n",
    "    pipeline_file,\n",
    "    flow_chart=flowchart_file,\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "# Run the flow-chart pipeline\n",
    "ceci.run_pipeline(pipeline_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc951a8-1364-465e-864f-2ef7c650b4e8",
   "metadata": {},
   "source": [
    "Now we can have a look at the chart it has created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15ba31-e00f-46a2-9251-40e68a5e5309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(flowchart_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d6764-39a3-4419-a5a9-195a3a1a1ddd",
   "metadata": {},
   "source": [
    "The flowchart elements are classified as follows:\n",
    "\n",
    "- Red ellipses are pipeline stages, each of which is a python class.\n",
    "\n",
    "- Yellow boxes are pre-existing input data files.\n",
    "\n",
    "- Blue boxes are files created by the pipeline.\n",
    "\n",
    "We can say that it generally makes sense - the final output is a set of cluster shear index catalogs, just like before.\n",
    "\n",
    "Let's have a look at the pipeline information for this stage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e8626-9e04-4ff6-8e46-46b524fae9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(pipeline_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec67963-f92c-4520-9aa6-7ef7b9d866b7",
   "metadata": {},
   "source": [
    "This dictionary defines what pipeline stages are run, and how they are executed. You can see:\n",
    "\n",
    "- a list of stages to be run, including their parallelization.\n",
    "- site information showing how to run individual steps.\n",
    "- directories to put logs and outputs\n",
    "- launcher information on how to launch and manage the workflow\n",
    "- overall inputs to the pipeline\n",
    "\n",
    "Finally, the 'config' item points to another file that configures the individual pipeline stages:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba942a-b927-43d7-88b3-1a44397445b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(pipeline_config['config']) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5543cce4-efcb-40a7-9d04-33588e85e8f3",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "After a bit more waiting, the final background cluster selection should complete.\n",
    "\n",
    "We can again use our combined catalog to explore it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0155f-54f3-46fb-8c57-9d8462a850ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/cosmodc2/outputs-20deg2/cluster_shear_catalogs.hdf5\"):\n",
    "    raise RuntimeError(\"Please wait a bit longer for the pipeline to complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca74a3-b0a0-45e1-8234-310d6ddd98ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: fix finding all these automatically from the pipeline object\n",
    "ccc = txpipe.extensions.CombinedClusterCatalog(\n",
    "    shear_catalog=\"data/cosmodc2/20deg2/shear_catalog.hdf5\",\n",
    "    shear_tomography_catalog=\"data/cosmodc2/outputs-20deg2/shear_tomography_catalog.hdf5\",\n",
    "    cluster_catalog=\"./data/cosmodc2/20deg2/cluster_catalog.hdf5\",\n",
    "    cluster_shear_catalogs=\"data/cosmodc2/outputs-20deg2/cluster_shear_catalogs.hdf5\",\n",
    "    photoz_pdfs=\"data/cosmodc2/outputs-20deg2/source_photoz_pdfs.hdf5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799066ba-a120-494b-994b-9968ad314efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of clusters:\n",
    "ccc.ncluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6867f1-cb4a-475f-8b80-ef2a459c24fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# info about one cluster\n",
    "info = ccc.get_cluster_info(500)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55288a22-9345-4554-8d54-9b11b4e9cda9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the number of galaxies behind this cluster\n",
    "ccc.get_background_catalog_indexing(500)[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f72253-e286-4a9e-bb8e-f9b64382f9c2",
   "metadata": {},
   "source": [
    "Depending on the file system this can be slow ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff57bf-c9ce-4ac4-be0c-4bcd347cc0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the catalog for this cluster:\n",
    "cat = ccc.get_background_shear_catalog(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c33b6-026d-4d2a-8ded-7ac8eb2a1274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check that the positions make sense for cluster and galaxies\n",
    "plt.plot(cat['ra'], cat['dec'] ,',')\n",
    "plt.plot(info['ra'], info['dec'], 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb96d0-234c-4ac2-a967-825f973753d4",
   "metadata": {},
   "source": [
    "Hopefully this is the information you need to do the next steps, but let me know if not!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da8efc-6f9e-4aac-bd34-833c9ed21e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TXPipe-2023-Jul-12",
   "language": "python",
   "name": "txpipe-2023-jul-12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
