launcher:
    name: mini
    interval: 1.0
    
site:
    name: local #cori-interactive
    #image: joezuntz/txpipe
    max_threads: 2

modules: txpipe

# where to find any modules that are not in this repo,
# and any other code we need.
python_paths:
    - submodules/WLMassMap/python/desc/
    - submodules/TJPCov
    - submodules/FlexZPipe
    - submodules/RAIL

# Stages to run
stages:
    - name: TXSourceSelectorMetacal     # select and split objects into source bins
    - name: TXShearCalibration   # Calibrate and split the source sample tomographically
    - name: TXLensCatalogSplitter  # Split the lens sample tomographically
    - name: TXStarCatalogSplitter  # Split the star catalog into separate bins (psf/non-psf)
    #- name: TXMeanLensSelector  # select objects for lens bins
    #- name: PZRailTrainSource   # Train a photo-z estimator for the source sample
    #  threads_per_process: 2
    #- name: PZRailEstimateSource # Compute p(z) values for the source sample
    # Copy the p(z) values from the source to lens values
    # # If we had separate source and lens samples (as we do in real life) we would
    # # esimate the lens PZ separately.
    # # We could use this to train a separate estimator:
    # - name: PZRailTrainLens     # Train a photo-z estimator for the lens sample
    #   threads_per_process: 2
    # # and this to estimate the p(z) for the lenses separately:
    # # (in this laptop test the samples are the same, as this is faster)
    # - name: PZRailEstimateLens # Compute p(z) values for the lens sample
    #- name: TXRandomPhotozPDF
    #- name: PZRailEstimateLensFromSource 
    #- name: TXPhotozSourceStack  # Stack p(z) into n(z)
    #- name: TXPhotozLensStack    # Stack p(z) into n(z)
    #- name: TXExternalLensCatalogSplitter
    - name: TXMainMaps           # make source g1, g2 and lens n_gal maps
    - name: TXAuxiliarySourceMaps  # make PSF and flag maps
    - name: TXAuxiliaryLensMaps    # make depth and bright object maps
    - name: TXSimpleMask         # combine maps to make a simple mask
    - name: TXSourceNoiseMaps    # noise maps for sources using random rotations
    - name: TXLensNoiseMaps      # noise maps for lenses using density splits
    - name: TXDensityMaps        # turn mask and ngal maps into overdensity maps
    - name: TXMapPlots           # make pictures of all the maps
    - name: TXTracerMetadata     # collate metadata
    - name: TXRandomCat          # generate lens bin random catalogs
    - name: TXJackknifeCenters   # Split the area into jackknife regions
    - name: TXTwoPoint           # Compute real-space 2-point correlations
      threads_per_process: 2
    - name: TXBlinding           # Blind the data following Muir et al
      threads_per_process: 2
    - name: TXTwoPointTheoryReal # compute theory using CCL to save in sacc file and plot later
    - name: TXTwoPointPlots      # Make plots of 2pt correlations
    - name: TXSourceDiagnosticPlots    # Make a suite of diagnostic plots
    - name: TXLensDiagnosticPlots    # Make a suite of diagnostic plots
    - name: TXGammaTFieldCenters # Compute and plot gamma_t around center points
      threads_per_process: 2
    - name: TXGammaTStars  # Compute and plot gamma_t around bright stars
      threads_per_process: 2
    - name: TXGammaTRandoms      # Compute and plot gamma_t around randoms
      threads_per_process: 2
    - name: TXRoweStatistics     # Compute and plot Rowe statistics
      threads_per_process: 2
    - name: TXGalaxyStarDensity  # Compute and plot the star-galaxy density cross-correlation
    - name: TXGalaxyStarShear    # Compute and plot the star-galaxy shear cross-correlation
    - name: TXPSFDiagnostics     # Compute and plots other PSF diagnostics
    - name: TXBrighterFatterPlot # Make plots tracking the brighter-fatter effect
    - name: TXPhotozPlots        # Plot the bin n(z)
    - name: TXConvergenceMaps    # Make convergence kappa maps from g1, g2 maps
    - name: TXConvergenceMapPlots # Plot the convergence map
    - name: TXMapCorrelations    # plot the correlations between systematics and data
    - name: TXApertureMass        # Compute aperture-mass statistics
      threads_per_process: 2
    # Disabled since not yet synchronised with new Treecorr MPI
    # - name: TXSelfCalibrationIA   # Self-calibration intrinsic alignments of galaxies

    # Disabling these as they takes too long for a quick test.
    # The latter three need NaMaster
    - name: TXRealGaussianCovariance   # Compute covariance of real-space correlations
    - name: TXTwoPointFourier          # Compute power spectra C_ell
    - name: TXFourierNamasterCovariance # Compute the C_ell covariance
    - name: TXFourierTJPCovariance     # Compute the C_ell covariance with TJPCov 

# Where to put outputs
output_dir: data/desy1/outputs

# python modules to import to search for stages
modules: txpipe

# configuration settings
config: examples/desy1/config.yml

# These are overall inputs to the whole pipeline, not generated within it
inputs:
    # See README for paths to download these files
    shear_catalog: data/desy1/shear_catalog_desy1.h5
    photometry_catalog: data/desy1/photometry_catalog_desy1.h5
    lens_catalog: data/desy1/lens_catalog.hdf5
    lens_tomography_catalog: data/desy1/lens_tomography_catalog.hdf5
    shear_photoz_stack: data/desy1/shear_photoz_stack.hdf5
    lens_photoz_stack: data/desy1/lens_photoz_stack.hdf5
    #photoz_source_training: submodules/RAIL/tests/data/test_dc2_training_9816.hdf5
    #photoz_source_testing: submodules/RAIL/tests/data/test_dc2_validation_9816.hdf5
    calibration_table: data/example/inputs/sample_cosmodc2_w10year_errors.dat
    exposures: data/example/inputs/exposures.hdf5
    star_catalog: data/example/inputs/star_catalog.hdf5
    # This file comes with the code
    fiducial_cosmology: data/fiducial_cosmology.yml
    # For the self-calibration extension we are not using Random_cat_source for now
    # So it is set to Null, so the yaml intepreter returns a None value to python. 
    random_cats_source: Null

# if supported by the launcher, restart the pipeline where it left off
# if interrupted
resume: True
# where to put output logs for individual stages
log_dir: data/example/logs
# where to put an overall parsl pipeline log
pipeline_log: data/example/log.txt
